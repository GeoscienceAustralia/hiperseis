{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import base64\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import rf\n",
    "import rf.imaging\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import obspy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in interactive widgets capability. See https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seismic.receiver_fn.rf_util as rf_util\n",
    "import seismic.receiver_fn.rf_plot_utils as rf_plot_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = r\"..\\DATA\\OA_event_waveforms_for_rf_20170911T000036-20181128T230620_ZRT_td_rev3_qual.h5\"\n",
    "# src_file = r\"..\\DATA\\OA_event_waveforms_for_rf_20170911T000036-20181128T230620_LQT_td_rev3_qual.h5\"\n",
    "training_station = 'BT23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_all = rf_util.read_h5_rf(src_file, network='OA', station=training_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(oa_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = rf_util.rf_to_dict(oa_all)\n",
    "oa_trainer = db[training_station]\n",
    "channel = 'HHR'\n",
    "# channel = 'HHQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oa_trainer[channel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional statistics for prediction of trace quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_util.compute_extra_rf_stats(oa_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display ranges of metadata and quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_series(traces, field):\n",
    "    x = [tr.stats.get(field) for tr in traces]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata and quality data on all traces for the target channel\n",
    "channel_data = oa_trainer[channel]\n",
    "\n",
    "snr = get_metadata_series(channel_data, 'snr')\n",
    "entropy = get_metadata_series(channel_data, 'entropy')\n",
    "coherence = get_metadata_series(channel_data, 'max_coherence')\n",
    "distance = get_metadata_series(channel_data, 'distance')\n",
    "inclination = get_metadata_series(channel_data, 'inclination')\n",
    "magnitude = get_metadata_series(channel_data, 'event_magnitude')\n",
    "depth = get_metadata_series(channel_data, 'event_depth')\n",
    "rf_group = get_metadata_series(channel_data, 'rf_group')\n",
    "# Replace no-group group IDs with '-1'\n",
    "rf_group = [g if g is not None else -1 for g in rf_group]\n",
    "amax = get_metadata_series(channel_data, 'amax')\n",
    "# Extra metrics\n",
    "amp_20pc = get_metadata_series(channel_data, 'amp_20pc')\n",
    "amp_80pc = get_metadata_series(channel_data, 'amp_80pc')\n",
    "mean_cplx_amp = get_metadata_series(channel_data, 'mean_cplx_amp')\n",
    "rms_amp = get_metadata_series(channel_data, 'rms_amp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate array of data and labels to use to plot histogram distributions\n",
    "dist_array = [(snr, \"SNR\"), (entropy, \"Entropy\"), (coherence, \"Coherence\"), (distance, \"Distance\"),\n",
    "              (inclination, \"Inclination\"), (magnitude, \"Magnitude\"), (amax, \"Max amplitude\"), (amp_20pc, \"Amplitude 20th perc.\"),\n",
    "              (amp_80pc, \"Amplitude 80th perc.\"), (mean_cplx_amp, \"Mean amplitude\"), (rms_amp, \"RMS amplitude\"), (rf_group, \"Group ID\")]\n",
    "# dist_array = [(snr, \"SNR\"), (entropy, \"Entropy\"), (coherence, \"Coherence\"), (distance, \"Distance\"),\n",
    "#               (inclination, \"Inclination\"), (magnitude, \"Magnitude\"), (amax, \"Max amplitude\"), (rf_group, \"Group ID\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of metrics\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.subplot(3,4,1)\n",
    "for i, (data, name) in enumerate(dist_array):\n",
    "    ax = plt.subplot(3, 4, i + 1)\n",
    "#     plt.hist(data, bins=20)\n",
    "    sns.distplot(data, bins=20, ax=ax)\n",
    "    plt.title(name + \" distribution\", y=0.88, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine co-plots to look for discriminating variables\n",
    "df = pd.DataFrame.from_dict({\"SNR\": snr, \"Entropy\": entropy, \"Coherence\": coherence, \"Max_amp\": amax,\n",
    "                             \"Amp_20pc\": amp_20pc, \"Amp_80pc\": amp_80pc, \"RMS_amp\": rms_amp, \"Mean_amp\": mean_cplx_amp,\n",
    "                             \"Magnitude\": \">=6\", \"Distance\": \">=60\", \"Depth\": \">=80km\",\n",
    "                             \"Inclination\": \">=20\", \"Group_id\": rf_group,\n",
    "                             \"Quality\": \"unknown\"})\n",
    "df.loc[(np.array(magnitude) < 6.0), \"Magnitude\"] = \"<6\"\n",
    "df.loc[(np.array(distance) < 60.0), \"Distance\"] = \"<60\"\n",
    "df.loc[(np.array(inclination) < 20.0), \"Inclination\"] = \"<20\"\n",
    "df.loc[(np.array(depth) < 80.0), \"Depth\"] = \"<80km\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_file = training_station + \"_quality_labels_ZRT.csv\"\n",
    "# qual_file = training_station + \"_quality_labels_LQT.csv\"\n",
    "if os.path.isfile(qual_file):\n",
    "    loaded_quality = pd.read_csv(qual_file, index_col=0, header=None)\n",
    "    df['Quality'] = loaded_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use interactive widget to manually label the quality of the traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quality guide:\")\n",
    "print(\"'a' = low signal before onset, higher signal after onset with some multiples visible\")\n",
    "print(\"'b' = signal similar before and after onset, cannot make out multiples with much confidence\")\n",
    "print(\"Create labels by entering 10 character string of 'a's and 'b's according to quality, ordered from bottom to top trace.\")\n",
    "# Create labels for quality. Note that rf plots are numbered from the bottom up, whereas the Pandas table is displayed ordered from the top down.\n",
    "quality_updated = False\n",
    "for i in range(0, len(df), 10):\n",
    "    existing_qual = df['Quality'].iloc[i:i+10].values\n",
    "    if not 'unknown' in existing_qual:\n",
    "        continue\n",
    "    rf_slice = rf.RFStream(channel_data[i:i+10])\n",
    "    rf_plot_utils.plot_rf_stack(rf_slice, trace_height=0.4)\n",
    "    plt.show()\n",
    "    get_labels = ''\n",
    "    quit = False\n",
    "    while len(get_labels) != len(rf_slice):\n",
    "        get_labels = input(\"Enter labels: \")\n",
    "        if get_labels.lower() == 'quit':\n",
    "            quit = True\n",
    "            break\n",
    "        if len(get_labels) != len(rf_slice):\n",
    "            print(\"Wrong number of labels, try again!\")\n",
    "    if quit:\n",
    "        break\n",
    "    for j, qual in enumerate(get_labels):\n",
    "        df['Quality'].iloc[i+j] = qual\n",
    "    quality_updated = True\n",
    "    display(df.iloc[i:i+10])\n",
    "\n",
    "if quality_updated:\n",
    "    df['Quality'].to_csv(qual_file)\n",
    "else:\n",
    "    display(df.sample(20, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign quality category to trace metadata\n",
    "for i, tr in enumerate(channel_data):\n",
    "    tr.stats.quality = df['Quality'].iloc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot labelled data to find metrics to discriminate trace quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_metrics = [\"SNR\", \"Entropy\", \"Coherence\", \"Max_amp\", \"Amp_20pc\", \"Amp_80pc\", \"RMS_amp\", \"Mean_amp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_pairplot(df, plot_vars, hue_by='Quality', title=''):\n",
    "    hue_order = None\n",
    "    if hue_by == 'Quality' or hue_by == 'Prediction':\n",
    "        hue_order = ['unknown', 'b', 'a'] if 'unknown' in df['Quality'] else ['b', 'a']\n",
    "    sns.pairplot(df, hue=hue_by, hue_order=hue_order, vars=plot_vars)\n",
    "    plt.suptitle(title, y=1.01, fontsize=20)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual\n",
    "def _metrics_pairplot(hue_by=['Quality', 'Magnitude', 'Distance', 'Depth', 'Inclination', 'Group_id']):\n",
    "    metrics_pairplot(df, stats_metrics, hue_by, title=\"Pairwise quality metrics scatter plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to manually select metadata metrics for filtering to the Quality A set of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total = len(channel_data)\n",
    "\n",
    "rf_data = [tr for tr in channel_data if tr.stats.quality == 'a']\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream_A = rf.RFStream(rf_data)\n",
    "print(\"Quality A: {} events\".format(len(rf_stream_A)))\n",
    "quality_A_ids = [tr.stats.event_id for tr in rf_stream_A]\n",
    "not_quality_A_ids = [tr.stats.event_id for tr in channel_data if tr.stats.event_id not in quality_A_ids]\n",
    "\n",
    "rf_data = [tr for tr in channel_data if tr.stats.snr >= 2.0 and tr.stats.entropy >= 3.5 and tr.stats.max_coherence >= 0.25]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream_stats_filtered = rf.RFStream(rf_data)\n",
    "num_filtered = len(rf_stream_stats_filtered)\n",
    "print(\"Stats filtered: {} events\".format(num_filtered))\n",
    "stats_filtered_ids = [tr.stats.event_id for tr in rf_stream_stats_filtered]\n",
    "true_positives = [id for id in stats_filtered_ids if id in quality_A_ids]\n",
    "false_negatives = [id for id in quality_A_ids if id not in stats_filtered_ids]\n",
    "num_true_positive = len(true_positives)\n",
    "num_false_negative = len(false_negatives)\n",
    "num_predicted_positive = len(stats_filtered_ids)\n",
    "num_predicted_negative = num_total - num_predicted_positive\n",
    "\n",
    "# Determine how many of the events in stats_filtered_ids are Quality A events\n",
    "print(\"{}/{} correct filtered events (snr, entropy, coherence) (Positive predictive value = {:.2f}%, False omission rate = {:.2f}%)\"\n",
    "      .format(num_true_positive, num_filtered, 100.0*num_true_positive/num_predicted_positive, 100*num_false_negative/num_predicted_negative))\n",
    "\n",
    "# Repeat using amplitude metrics\n",
    "rf_data = [tr for tr in channel_data if tr.stats.amax <= 0.3 and tr.stats.amp_20pc <= 0.03 and tr.stats.amp_80pc <= 0.1]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream_stats2_filtered = rf.RFStream(rf_data)\n",
    "num2_filtered = len(rf_stream_stats2_filtered)\n",
    "print(\"Stats2 filtered: {} events\".format(num2_filtered))\n",
    "stats2_filtered_ids = [tr.stats.event_id for tr in rf_stream_stats2_filtered]\n",
    "true_positives = [id for id in stats2_filtered_ids if id in quality_A_ids]\n",
    "false_negatives = [id for id in quality_A_ids if id not in stats2_filtered_ids]\n",
    "num_true_positive = len(true_positives)\n",
    "num_false_negative = len(false_negatives)\n",
    "num_predicted_positive = len(stats2_filtered_ids)\n",
    "num_predicted_negative = num_total - num_predicted_positive\n",
    "\n",
    "print(\"{}/{} filtered events (Max. amp, 20%, 80%) are quality A events (Positive predictive value = {:.2f}%, False omission rate = {:.2f}%)\"\n",
    "      .format(num_true_positive, num2_filtered, 100.0*num_true_positive/num_predicted_positive, 100*num_false_negative/num_predicted_negative))\n",
    "\n",
    "# The performance stats shown below show what a human achieves trying to tune data selection criteria manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how well a neural network classifier works in comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use simple stats for feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_quality_mask = (df['Quality'] != 'unknown')\n",
    "X = df.loc[known_quality_mask, stats_metrics]\n",
    "X[np.isnan(X)] = 0\n",
    "y = df['Quality'].loc[known_quality_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# X_train_transformed = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This perceptron network has been simplified back to the bare bone so that it corresponds to a linear predictor,\n",
    "# as higher order complexity and non-linear activation functions gave no improvement in accuracy.\n",
    "clf_simple = MLPClassifier(solver='lbfgs', alpha=1e-4, max_iter=1000, activation='identity',\n",
    "                           hidden_layer_sizes=(1,), random_state=3772, tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation to tune hyperparameters\n",
    "scores = cross_val_score(clf_simple, X_train, y_train, cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With tuned hyperparameters, train on full training set.\n",
    "clf_simple.fit(X_train, y_train)\n",
    "print(\"Final loss: %0.4f\" % clf_simple.loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = clf_simple.score(X_test, y_test)\n",
    "print(\"Final accuracy: %0.3f\" % final_score)\n",
    "# We get decent performance with a trivial network (1 neuron) with trivial activation f(x) = x,\n",
    "# which means that simply a linear combination of feature vector is sufficient to determine\n",
    "# classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply quality classifier to whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prediction = clf_simple.predict(X)\n",
    "confidences = clf_simple.predict_proba(X)\n",
    "confidence_index = np.zeros(full_prediction.shape).astype(np.int)\n",
    "confidence_index[(full_prediction == 'b')] = 1\n",
    "df.loc[known_quality_mask, 'Prediction'] = full_prediction\n",
    "df.loc[known_quality_mask, 'Confidence'] = confidences[range(confidence_index.size), confidence_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the order of metrics so that weightings can be interpreted in relation to metrics\n",
    "print(stats_metrics)\n",
    "# Display the coefficients of the trained classifier\n",
    "print(\"Hidden layer:\")\n",
    "print(\"  weightings: {}\".format(clf_simple.coefs_[0].T[0]))\n",
    "print(\"        bias: {}\".format(clf_simple.intercepts_[0]))\n",
    "A0 = clf_simple.coefs_[0].T[0]\n",
    "b0 = clf_simple.intercepts_[0][0]\n",
    "\n",
    "print(\"Output layer:\")\n",
    "print(\"  weightings: {}\".format(clf_simple.coefs_[1][0]))\n",
    "print(\"        bias: {}\".format(clf_simple.intercepts_[1]))\n",
    "A1 = clf_simple.coefs_[1][0][0]\n",
    "b1 = clf_simple.intercepts_[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_metric = A1*(np.matmul(X.values, A0) + b0) + b1\n",
    "df.loc[known_quality_mask, 'Prediction metric'] = prediction_metric\n",
    "df.sample(20, random_state=3772)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot whole prediction dataset\n",
    "@interact_manual\n",
    "def _metrics_pairplot(hue_by=['Prediction', 'Quality']):\n",
    "    metrics_pairplot(df, stats_metrics, hue_by, title=\"Pairwise predicted quality scatter plot - full data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction just on test dataset\n",
    "@interact_manual\n",
    "def _metrics_pairplot(hue_by=['Prediction', 'Quality']):\n",
    "    metrics_pairplot(df.iloc[sorted(X_test.index)], stats_metrics, hue_by, title=\"Pairwise predicted quality scatter plot - test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix and verify how to compute accuracy from it.\n",
    "cm = confusion_matrix(df.loc[known_quality_mask, 'Quality'], df.loc[known_quality_mask, 'Prediction'], labels=['b', 'a'])\n",
    "print(cm)\n",
    "print(np.sum(cm))\n",
    "print(\"Accuracy: %0.3f\" % (np.sum(np.diag(cm))/float(np.sum(cm))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how good is the DBSCAN grouping as an indicator of trace quality.\n",
    "dbscan_group = df['Group_id'].copy()\n",
    "primary_group_mask = (dbscan_group == 0)\n",
    "dbscan_group[primary_group_mask] = 'a'\n",
    "dbscan_group[~primary_group_mask] = 'b'\n",
    "cm_dbscan = confusion_matrix(df.loc[known_quality_mask, 'Quality'], dbscan_group[known_quality_mask], labels=['b', 'a'])\n",
    "print(cm_dbscan)\n",
    "print(np.sum(cm_dbscan))\n",
    "print(\"Accuracy: %0.3f\" % (np.sum(np.diag(cm_dbscan))/float(np.sum(cm_dbscan))))\n",
    "# Result here indicates DBSCAN grouping is not a strong predictor of subjective trace quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how good SNR alone is as an indicator of trace quality.\n",
    "snr_series = df['SNR'].copy()\n",
    "high_snr_mask = (snr_series >= 1.5)\n",
    "snr_series[high_snr_mask] = 'a'\n",
    "snr_series[~high_snr_mask] = 'b'\n",
    "cm_snr = confusion_matrix(df.loc[known_quality_mask, 'Quality'], snr_series[known_quality_mask], labels=['b', 'a'])\n",
    "print(cm_snr)\n",
    "print(np.sum(cm_snr))\n",
    "print(\"Accuracy: %0.3f\" % (np.sum(np.diag(cm_snr))/float(np.sum(cm_snr))))\n",
    "# Result here indicates SNR alone is quite a good indicator of quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist classifier model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = training_station + \"_classifier_ZRT.json\"\n",
    "# model_file = training_station + \"_classifier_LQT.json\"\n",
    "model = {}\n",
    "model['params'] = clf_simple.get_params()\n",
    "model['coeffs'] = base64.b64encode(pkl.dumps(clf_simple.coefs_, pkl.HIGHEST_PROTOCOL)).decode('utf-8')\n",
    "model['biases'] = base64.b64encode(pkl.dumps(clf_simple.intercepts_, pkl.HIGHEST_PROTOCOL)).decode('utf-8')\n",
    "model['binarizer'] = base64.b64encode(pkl.dumps(clf_simple._label_binarizer, pkl.HIGHEST_PROTOCOL)).decode('utf-8')\n",
    "model['classes'] = clf_simple.classes_.tolist()\n",
    "model['out_activation'] = clf_simple.out_activation_\n",
    "model['n_outputs'] = clf_simple.n_outputs_\n",
    "model['n_layers'] = clf_simple.n_layers_\n",
    "with open(model_file, 'w') as f:\n",
    "    json.dump(model, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
