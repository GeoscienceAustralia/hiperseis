{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3d1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import Geod\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import cartopy.crs as ccrs\n",
    "from copy import deepcopy\n",
    "from obspy.core import UTCDateTime, Trace\n",
    "from collections import defaultdict\n",
    "from obspy.core import  read\n",
    "from obspy.geodetics.base import degrees2kilometers\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib \n",
    "from matplotlib.figure import Figure\n",
    "import sys, os\n",
    "import json\n",
    "import obspy\n",
    "from obspy import read_inventory\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.core.util import AttribDict\n",
    "from obspy.core.inventory.util import Equipment\n",
    "sys.path.append('/g/data/ha3/rakib/seismic/pst/hiperseis')\n",
    "\n",
    "from seismic.ASDFdatabase.FederatedASDFDataSet import FederatedASDFDataSet\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e365c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/data/ha3/Passive/SHARED_ENV/hiperseis_gadi/lib64/python3.6/site-packages/obspy/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(obspy.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c713500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found database: 13274e5d8e6db92411193923a275458638c15f59.db\n"
     ]
    }
   ],
   "source": [
    "fds = FederatedASDFDataSet('asdf_files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b76bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "def load_corrections():\n",
    "    correction_map = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "    pattern = os.path.join(os.path.dirname(fds.asdf_source), '.corrections/*.csv')\n",
    "    fnames = glob.glob(pattern)\n",
    "\n",
    "    if(len(fnames)): print('Loading corrections..')\n",
    "\n",
    "    dtypes = {'net':str, 'sta':str, 'loc':str, 'comp':str, 'date':str,\n",
    "              'clock_correction':str}\n",
    "    for fname in fnames:\n",
    "        df = pd.read_csv(fname, delimiter=',', header=0, dtype=dtypes, na_filter=False)\n",
    "\n",
    "        try:\n",
    "            corr_count = 0\n",
    "            for i in np.arange(len(df)):\n",
    "                net = df['net'][i]\n",
    "                sta = df['sta'][i]\n",
    "                loc = df['loc'][i]\n",
    "                corr = df['clock_correction'][i]\n",
    "\n",
    "                if(corr == 'NOXCOR'): continue\n",
    "                else: corr = float(df['clock_correction'][i])\n",
    "\n",
    "                st = UTCDateTime(df['date'][i]).timestamp\n",
    "                et = st + 24*3600\n",
    "\n",
    "                correction_map[net][sta][loc].append([st, et, corr])\n",
    "                corr_count += 1\n",
    "            # end for\n",
    "        except Exception as e:\n",
    "            print ('Warning: failed to read corrections file {} with error({}). '\n",
    "                   'Continuing along..'.format(fname, traceback.format_exc()))\n",
    "        #end try\n",
    "    # end for\n",
    "    \n",
    "    return correction_map\n",
    "#end func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2534c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corrections..\n"
     ]
    }
   ],
   "source": [
    "clock_corrs = load_corrections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9e3585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corrections..\n",
      "No RF correction found for station OA.BS24.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_orientations_corrections(path_to_files):\n",
    "    rf_correction_map = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    swp_correction_map = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "    pattern = os.path.join(path_to_files, '*.json')\n",
    "    fnames = glob.glob(pattern)\n",
    "\n",
    "    if(len(fnames)): print('Loading corrections..')\n",
    "\n",
    "    for fname in fnames:\n",
    "        orientations = json.load(open(fname))\n",
    "\n",
    "        try:\n",
    "            rf_dict = orientations['rf']\n",
    "            swp_dict = orientations['swp']\n",
    "\n",
    "            for k,v in rf_dict.items():\n",
    "                try:\n",
    "                    net, sta, loc = k.split('.')\n",
    "                    st, et = v['date_range']\n",
    "                    az_corr = v['azimuth_correction']\n",
    "                    rf_correction_map[net][sta][loc].append([st, et, az_corr])\n",
    "                except:\n",
    "                    print('No RF correction found for station {}'.format(k))\n",
    "                # end if\n",
    "            # end for\n",
    "            \n",
    "            for k,v in swp_dict.items():\n",
    "                try:\n",
    "                    net, sta, loc = k.split('.')\n",
    "                    st, et = v['date_range']\n",
    "                    az_corr = v['azimuth_correction']\n",
    "                    az_uncert = v['uncertainty']\n",
    "                    swp_correction_map[net][sta][loc].append([st, et, az_corr, az_uncert])\n",
    "                except:\n",
    "                    print('No SWP correction found for station {}'.format(k))\n",
    "                # end if                    \n",
    "            # end for            \n",
    "        except Exception as e:\n",
    "            print ('Warning: failed to read corrections file {} with error({}). '\n",
    "                   'Continuing along..'.format(fname, traceback.format_exc()))\n",
    "        #end try\n",
    "    # end for\n",
    "    \n",
    "    return rf_correction_map, swp_correction_map\n",
    "# end func\n",
    "\n",
    "rf_orientations, swp_orientations = \\\n",
    "load_orientations_corrections('orientations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d3bfcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Response\n",
      "\tFrom M/S (Velocity in Meters per Second) to COUNTS (Digital Counts)\n",
      "\tOverall Sensitivity: 3.09063e+08 defined at 1.000 Hz\n",
      "\t2 stages:\n",
      "\t\tStage 1: PolesZerosResponseStage from M/S to V, gain: 754.3\n",
      "\t\tStage 2: CoefficientsTypeResponseStage from V to COUNTS, gain: 409735\n"
     ]
    }
   ],
   "source": [
    "# get response object\n",
    "resp_file = '/g/data/ha3/Passive/SHARED_DATA/Inventory/Station_Extra_Metadata/Equipments/RESP.COMPACT120S.MINIMUS.txt'\n",
    "resp_inv = obspy.read_inventory(resp_file, format='RESP')\n",
    "\n",
    "datetime = UTCDateTime(\"2015-01-01T00:00:00\")\n",
    "resp_obj = resp_inv.get_response(\"XX.XX..BHZ\", datetime)\n",
    "print(resp_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c522327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inventory from fds and create a merged inventory with responses and corrections\n",
    "inv = fds.get_inventory(network='OA')\n",
    "\n",
    "# keep stations are until 2019-12-31 (removed CA45 and BQ42 from sqlite output)\n",
    "nslc_list = open('stations_to_2019_12_31.txt').readlines()\n",
    "nslc_list = set([item.strip() for item in nslc_list])\n",
    "\n",
    "ns_set = set()\n",
    "for nslc in nslc_list:\n",
    "    net,sta,loc,cha = nslc.split('.')\n",
    "    ns_set.add('{}.{}'.format(net, sta))\n",
    "# end for\n",
    "\n",
    "oinv = None\n",
    "for ns in ns_set:\n",
    "    net,sta = ns.split('.')\n",
    "    if(oinv is None): oinv = inv.select(network=net, station=sta).copy()\n",
    "    else: \n",
    "        cinv = inv.select(network=net, station=sta).copy()\n",
    "        for net in cinv.networks:\n",
    "            for sta in net.stations:\n",
    "                oinv.networks[0].stations.append(sta)\n",
    "                if(oinv.networks[0].start_date > sta.start_date): \n",
    "                    oinv.networks[0].start_date = sta.start_date\n",
    "                if(oinv.networks[0].end_date < sta.end_date): \n",
    "                    oinv.networks[0].end_date = sta.end_date\n",
    "# end for\n",
    "\n",
    "# add responses and equipment\n",
    "for net in oinv.networks:\n",
    "    for sta in net.stations:\n",
    "        for cha in sta.channels:\n",
    "            cha.response = resp_obj\n",
    "            \n",
    "            sensor = Equipment(type='Sensor', \n",
    "                               description='Nanometrics Trillium Compact 120s')\n",
    "            digitizer = Equipment(type='Digitizer', \n",
    "                               description='Guralp Minimus')\n",
    "            \n",
    "            cha.equipments = []\n",
    "            cha.equipments.append(sensor) \n",
    "            cha.equipments.append(digitizer) \n",
    "        # end for\n",
    "    # end for\n",
    "# end for\n",
    "\n",
    "# define namespace\n",
    "fmt=\"%Y-%m-%dT%H:%M:%S\"\n",
    "nsmap = {}\n",
    "nsmap['GeoscienceAustralia']= 'https://github.com/GeoscienceAustralia/hiperseis'\n",
    "# add corrections\n",
    "for net in oinv.networks:\n",
    "    for sta in net.stations:  \n",
    "        sta.extra = AttribDict()\n",
    "\n",
    "        # add gps clock corrections\n",
    "        if(len(clock_corrs[net.code][sta.code])):\n",
    "            ad = AttribDict()\n",
    "            ad.namespace = nsmap['GeoscienceAustralia']\n",
    "            \n",
    "            corr_dict = {}\n",
    "            for lk in clock_corrs[net.code][sta.code].keys():\n",
    "                corr_list = []\n",
    "                for row in clock_corrs[net.code][sta.code][lk]:\n",
    "                    st, et, corr = row\n",
    "                    key = '{} - {}'.format(UTCDateTime(st).strftime(fmt),\n",
    "                                           UTCDateTime(et).strftime(fmt))\n",
    "                    corr_list.append({key: corr})\n",
    "                # end for\n",
    "                corr_dict[lk] = corr_list\n",
    "            # end for\n",
    "            \n",
    "            ad.value = json.dumps(corr_dict)\n",
    "            sta.extra.clock_corrections = ad\n",
    "        # end if        \n",
    "        \n",
    "        # add rf orientation corrections\n",
    "        if(len(rf_orientations[net.code][sta.code])):\n",
    "            ad = AttribDict()\n",
    "            ad.namespace = nsmap['GeoscienceAustralia']\n",
    "            \n",
    "            corr_dict = {}\n",
    "            for lk in rf_orientations[net.code][sta.code].keys():\n",
    "                st, et, corr = rf_orientations[net.code][sta.code][lk][0]\n",
    "                key = '{} - {}'.format(UTCDateTime(st).strftime(fmt),\n",
    "                                       UTCDateTime(et).strftime(fmt))\n",
    "                corr_dict[lk] = {key: corr}\n",
    "            # end for\n",
    "            \n",
    "            ad.value = json.dumps(corr_dict)\n",
    "            sta.extra.rf_orientation_corrections = ad\n",
    "        # end if                \n",
    "        \n",
    "        # add swp orientation corrections\n",
    "        if(len(swp_orientations[net.code][sta.code])):\n",
    "            ad = AttribDict()\n",
    "            ad.namespace = nsmap['GeoscienceAustralia']\n",
    "            \n",
    "            corr_dict = {}\n",
    "            for lk in swp_orientations[net.code][sta.code].keys():\n",
    "                st, et, corr, corr_uncert = swp_orientations[net.code][sta.code][lk][0]\n",
    "                key = '{} - {}'.format(UTCDateTime(st).strftime(fmt),\n",
    "                                       UTCDateTime(et).strftime(fmt))\n",
    "                corr_dict[lk] = {key: [round(corr, 1), round(corr_uncert, 1)]}\n",
    "            # end for\n",
    "            \n",
    "            ad.value = json.dumps(corr_dict)\n",
    "            sta.extra.swp_orientation_corrections = ad\n",
    "        # end if                        \n",
    "    # end for\n",
    "# end for\n",
    "\n",
    "#print(oinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a9455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oinv.write('OA.xml', format='STATIONXML', nsmap=nsmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8d30ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot map of stations\n",
    "\n",
    "netstas = []\n",
    "lons = [] \n",
    "lats = []\n",
    "\n",
    "for net in oinv.networks:\n",
    "    for sta in net.stations:\n",
    "        netstas.append('{}.{}'.format(net.code, sta.code))\n",
    "        lons.append(sta.longitude)\n",
    "        lats.append(sta.latitude)\n",
    "    # end for\n",
    "# end for\n",
    "\n",
    "lons = np.array(lons)\n",
    "lats = np.array(lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfaf35e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/ha3/Passive/SHARED_ENV/hiperseis_gadi/lib64/python3.6/site-packages/cartopy/crs.py:228: ShapelyDeprecationWarning: __len__ for multi-part geometries is deprecated and will be removed in Shapely 2.0. Check the length of the `geoms` property instead to get the  number of parts of a multi-part geometry.\n",
      "  if len(multi_line_string) > 1:\n",
      "/g/data/ha3/Passive/SHARED_ENV/hiperseis_gadi/lib64/python3.6/site-packages/cartopy/crs.py:280: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for line in multi_line_string:\n",
      "/g/data/ha3/Passive/SHARED_ENV/hiperseis_gadi/lib64/python3.6/site-packages/cartopy/crs.py:347: ShapelyDeprecationWarning: __len__ for multi-part geometries is deprecated and will be removed in Shapely 2.0. Check the length of the `geoms` property instead to get the  number of parts of a multi-part geometry.\n",
      "  if len(p_mline) > 0:\n"
     ]
    }
   ],
   "source": [
    "clon = np.mean(lons)\n",
    "crs = ccrs.PlateCarree(central_longitude=clon)\n",
    "\n",
    "left = 0.05\n",
    "bottom = 0.05\n",
    "width = 0.9\n",
    "height = 0.8\n",
    "\n",
    "fig = Figure(figsize=(12, 5))\n",
    "ax = fig.add_axes([left, bottom, width, height], projection=crs)\n",
    "\n",
    "\n",
    "# draw coastlines.\n",
    "ax.coastlines('50m')\n",
    "gl = ax.gridlines(crs=crs, draw_labels=True,\n",
    "                  dms=False,\n",
    "                  linewidth=1, color='gray',\n",
    "                  alpha=0.5, linestyle='--')\n",
    "# plot stations\n",
    "for i, sc in enumerate(netstas):\n",
    "    lon, lat = lons[i], lats[i]\n",
    "\n",
    "    px, py = lon, lat\n",
    "    pxl, pyl = lon + 0.02, lat - 0.1\n",
    "    ax.scatter(px, py, 20, transform=crs, marker='v', c='r', edgecolor='none', zorder=1)\n",
    "    ax.annotate(sc.split('.')[1], xy=(pxl, pyl), fontsize=5, zorder=2)\n",
    "# end for\n",
    "fig.savefig('OA_stations.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
