{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze a cross-correlation to produce station clock correction file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to using this script, the quality of the correction should be visualized and confirmed using notebook `plotStationPairXcorr.ipynb`\n",
    "\n",
    "This notebook will generate a csv file with dates and estimated clock corrections for a given station. Applying the correction to the original ASDF database will be done separately for the sake of safety, so that any changes to ASDF must be very deliberate and intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.dates\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil import rrule\n",
    "import pandas as pd\n",
    "from scipy.interpolate import UnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_root = os.path.abspath('../../..')\n",
    "if package_root not in sys.path:\n",
    "    sys.path.append(package_root)\n",
    "from seismic.xcorqc.xcorr_station_clock_analysis import (XcorrClockAnalyzer, \n",
    "                                                         plot_estimated_timeshift)\n",
    "from seismic.xcorqc.analytic_plot_utils import timestamps_to_plottable_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FILE = \"/g/data/ha3/Passive/SHARED_DATA/GPS_Clock/xcorr/AU/HTT_STKA/test/AU.HTT.AU.STKA.nc\"\n",
    "# SRC_FILE = \"/g/data/ha3/Passive/SHARED_DATA/GPS_Clock/xcorr/7X/MA43_QIS/7X.MA43.AU.QIS.1.0-10.0.nc\"\n",
    "# SRC_FILE = \"/g/data/ha3/Passive/SHARED_DATE/GPS_Clock/xcorr/7X/MA52_QIS/7X.MA52.AU.QIS.1.0-10.0.nc\"\n",
    "assert os.path.exists(SRC_FILE), \"File not found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, basename = os.path.split(SRC_FILE)\n",
    "_, file_type = os.path.splitext(SRC_FILE)\n",
    "name_parts = basename.split('.')\n",
    "NETCODE = name_parts[0]\n",
    "STATCODE = name_parts[1]\n",
    "print(\"Inferred target station code: {}.{}\".format(NETCODE, STATCODE))\n",
    "FULL_CODE = '.'.join([NETCODE, STATCODE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 300 # +/-\n",
    "SNR_THRESHOLD = 6\n",
    "PCF_CUTOFF_THRESHOLD = 0.5\n",
    "\n",
    "xcorr_preproc = XcorrClockAnalyzer(SRC_FILE, TIME_WINDOW, SNR_THRESHOLD, PCF_CUTOFF_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment the corrections time series into coherent groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_coeffs = {\n",
    "    '7X.MA43': (1, 1, 20),\n",
    "    '7X.MA52': (1, 5, 15),\n",
    "    '7D.DA41A': (0.4, 0.3, 0.0),\n",
    "    'AU.HTT': (1, 1, 0)\n",
    "}\n",
    "\n",
    "assert FULL_CODE in tuned_coeffs, \"Add new coefficients for {}, then manually tune for fit\".FULL_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_coeffs = tuned_coeffs[FULL_CODE]\n",
    "ind, ids = xcorr_preproc.do_clustering(cluster_coeffs)\n",
    "num_segments = len(set(ids[ids != -1]))\n",
    "print(\"{} clusters identified\".format(num_segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot clusters based on sample positions in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "ax = plt.gca()\n",
    "\n",
    "xcorr_preproc.plot_clusters(ax, ids, cluster_coeffs, FULL_CODE)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.savefig(FULL_CODE + \"_clustering_profile.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With successful segmentation, we perform regression for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting univariate spline\n",
    "degree = dict(zip(range(num_segments), [1]*num_segments))\n",
    "regressions = xcorr_preproc.do_spline_regression(ids, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replot with fitted line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "ax = plt.gca()\n",
    "\n",
    "xcorr_preproc.plot_regressors(ax, ids, regressions, FULL_CODE)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.savefig(FULL_CODE + \"_regression_profile.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample regression lines to the original sample times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of daily spaced time values and computed correction, since source data time\n",
    "# points might not be uniformly distributed. Keyed by group ID. These are the times\n",
    "# at which we will output corrections.\n",
    "sec_per_day = 24*3600\n",
    "regular_corrections = xcorr_preproc.do_spline_resampling(ids, regressions, sec_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replot to sanity check the final daily correction values\n",
    "plt.figure(figsize=(16,9))\n",
    "ax = plt.gca()\n",
    "\n",
    "xcorr_preproc.plot_resampled_clusters(ax, ids, regular_corrections, FULL_CODE)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.savefig(FULL_CODE + \"_clock_correction_profile.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output regression results to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tabular format for ease of use and interoperability, even though there will be some redundancy of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_blocks = []\n",
    "for k in regular_corrections.keys():\n",
    "    c = regular_corrections[k]\n",
    "    # BEWARE: The 'corrections' array sign is negated there, since the correction\n",
    "    # we have computed up to this point is actually the clock *error*. Subtraction\n",
    "    # of an error is the same as addition of a correction of opposite sign.\n",
    "    data_blocks.append(pd.DataFrame(np.column_stack([c['times'], -c['corrections']]), \n",
    "                                    columns=['timestamp', 'clock_correction']))\n",
    "df = pd.concat(data_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['timestamp'].apply(obspy.UTCDateTime).apply(lambda x: x.date)\n",
    "df['net'] = NETCODE\n",
    "df['sta'] = STATCODE\n",
    "df = df[['net', 'sta', 'date', 'clock_correction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = FULL_CODE + \"_clock_correction.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exporting corrections to file {}\".format(output_file))\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
