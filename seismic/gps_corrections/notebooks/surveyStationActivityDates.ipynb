{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey ASDF database for daily activity for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import pytz\n",
    "import dateutil\n",
    "from dateutil import rrule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_root = os.path.abspath('../../..')\n",
    "if package_root not in sys.path:\n",
    "    sys.path.append(package_root)\n",
    "from seismic.ASDFdatabase import FederatedASDFDataSet\n",
    "import obspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_NETWORKS = ['AU', '7B', '7D', '7E', '7F', '7G', '7J', '7W', '7X', 'OA', 'S']\n",
    "TARGET_NETWORKS = ['AQ']\n",
    "\n",
    "pkl_filename = ','.join(TARGET_NETWORKS) + \"_station_survey.pkl\"\n",
    "\n",
    "regenerate_data = not os.path.exists(pkl_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full date ranges for temporary deployments\n",
    "TEMP_DEPLOYMENTS = {}\n",
    "TEMP_DEPLOYMENTS['7B'] = (obspy.UTCDateTime('1993-05-03T03:00:58.000000Z'),\n",
    "                          obspy.UTCDateTime('1995-08-10T01:25:16.000000Z'))\n",
    "TEMP_DEPLOYMENTS['7D'] = (obspy.UTCDateTime('2012-01-01T00:01:36.000000Z'),\n",
    "                          obspy.UTCDateTime('2014-03-27T15:09:51.000000Z'))\n",
    "TEMP_DEPLOYMENTS['7E'] = (obspy.UTCDateTime('1998-05-22T02:27:38.000000Z'),\n",
    "                          obspy.UTCDateTime('1998-07-25T02:09:54.000000Z'))\n",
    "TEMP_DEPLOYMENTS['7F'] = (obspy.UTCDateTime('2012-12-31T23:59:59.000000Z'),\n",
    "                          obspy.UTCDateTime('2014-11-15T00:43:14.000000Z'))\n",
    "TEMP_DEPLOYMENTS['7G'] = (obspy.UTCDateTime('2014-01-01T00:00:06.000000Z'),\n",
    "                          obspy.UTCDateTime('2016-02-09T21:04:29.000000Z'))\n",
    "TEMP_DEPLOYMENTS['7J'] = (obspy.UTCDateTime('2006-02-04T03:12:15.000000Z'),\n",
    "                          obspy.UTCDateTime('2007-05-30T23:59:59.000000Z'))\n",
    "TEMP_DEPLOYMENTS['7W'] = (obspy.UTCDateTime('2008-08-27T03:27:12.000000Z'),\n",
    "                          obspy.UTCDateTime('2011-05-24T00:34:59.000000Z'))\n",
    "TEMP_DEPLOYMENTS['7X'] = (obspy.UTCDateTime('2009-06-16T03:42:00.000000Z'),\n",
    "                          obspy.UTCDateTime('2011-04-01T23:18:49.000000Z'))\n",
    "TEMP_DEPLOYMENTS['OA'] = (obspy.UTCDateTime('2017-09-13T23:59:13.000000Z'),\n",
    "                          obspy.UTCDateTime('2018-11-28T01:11:14.000000Z'))\n",
    "TEMP_DEPLOYMENTS['AQ'] = (obspy.UTCDateTime('2015-11-28T04:20:00.000000Z'),\n",
    "                          obspy.UTCDateTime('2018-01-14T01:18:49.000000Z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_data:\n",
    "    ds = FederatedASDFDataSet.FederatedASDFDataSet(\"/g/data/ha3/Passive/SHARED_DATA/Index/asdf_files.txt\",\n",
    "                                                   variant='db')\n",
    "\n",
    "    # Get all nets and stations.\n",
    "    # Gets list of tuples containing [net, sta, start_time, end_time]\n",
    "    # Start- and end-times are instances of obspy UTCDateTime\n",
    "    import time\n",
    "    import sqlite3\n",
    "    tries = 10\n",
    "    while tries > 0:\n",
    "        tries = tries - 1\n",
    "        try:\n",
    "            all_codes = [x for x in ds.local_net_sta_list()]\n",
    "            ntries = 10 - tries\n",
    "            tries = 0\n",
    "            print(\"Success! ({} {})\".format(ntries, 'try' if ntries == 1 else 'tries'))\n",
    "        except sqlite3.DatabaseError:\n",
    "            if tries == 0:\n",
    "                raise\n",
    "            else:\n",
    "                print(\"Retrying station list load ({} tries remaining)...\".format(tries))\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_ds():\n",
    "    ds = FederatedASDFDataSet.FederatedASDFDataSet(\"/g/data/ha3/Passive/SHARED_DATA/Index/asdf_files.txt\", variant='db')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_data:\n",
    "    targets = [x for x in all_codes if x[0] in TARGET_NETWORKS]\n",
    "    len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PY2 = sys.version_info[0] < 3\n",
    "# Persist results to pickle file, since they take a couple of hours to generate.\n",
    "if PY2:\n",
    "    import cPickle as pkl\n",
    "else:\n",
    "    import pickle as pkl\n",
    "\n",
    "if regenerate_data: # This takes > 4 hours to run, only run when really needed.\n",
    "    print(\"Regenerating data from scratch, this could take hours...\")\n",
    "\n",
    "    import sqlite3\n",
    "\n",
    "    # Catalog the number of traces on a daily basis for all stations and channels\n",
    "    one_day_sec = 3600*24\n",
    "    millisec = 0.001\n",
    "\n",
    "    # Format of each result row: NET, STA, LOC, CHA, DATE, NUM_TRACES\n",
    "    temp_result = []\n",
    "    pbar = tqdm(total=len(targets))\n",
    "    for r in targets:\n",
    "        pbar.update()\n",
    "        net = r[0]\n",
    "        sta = r[1]\n",
    "        stn_start_time = r[2]\n",
    "        stn_end_time = r[3]\n",
    "        while True:\n",
    "            try:\n",
    "                stn_channels = ds.get_stations(stn_start_time, stn_end_time, network=net, station=sta)\n",
    "                break\n",
    "            except sqlite3.DatabaseError:\n",
    "                print(\"WARNING: database error (1) accessing {}.{}, sleeping then retrying...\".format(net, sta))\n",
    "                ds = reset_ds()\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "        loc_chan = [(s[2], s[3]) for s in stn_channels]\n",
    "        for loc, cha in loc_chan:\n",
    "            # TODO: Parallelize this loop by breaking up into N time epochs\n",
    "            pbar.set_description('Processing {}.{}.{}.{}'.format(net, sta, loc, cha))\n",
    "            while True:\n",
    "                try:\n",
    "                    start_time, end_time = ds.get_global_time_range(net, sta, loc, cha)\n",
    "                    if end_time.year == 2032:\n",
    "                        end_time.year = 2017\n",
    "                    break\n",
    "                except sqlite3.DatabaseError:\n",
    "                    print(\"WARNING: database error (2) accessing {}.{}, sleeping then retrying...\".format(loc, cha))\n",
    "                    ds = reset_ds()\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "            start_day = obspy.UTCDateTime(start_time.year, start_time.month, start_time.day)\n",
    "            end_day = obspy.UTCDateTime(end_time.year, end_time.month, end_time.day) + one_day_sec\n",
    "            day_begin = start_day\n",
    "            day_end = day_begin + one_day_sec - millisec\n",
    "            while True:\n",
    "                try:\n",
    "                    count = ds.get_waveform_count(net, sta, loc, cha, day_begin, day_end)\n",
    "                except sqlite3.DatabaseError:\n",
    "                    print(\"WARNING: database error (3) accessing {}, sleeping then retrying...\".format(str(day_begin)))\n",
    "                    ds = reset_ds()\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                temp_result.append([net, sta, loc, cha, day_begin, count])\n",
    "                if day_end > end_day:\n",
    "                    break\n",
    "                else:\n",
    "                    day_begin += one_day_sec\n",
    "                    day_end = day_begin + one_day_sec - millisec\n",
    "    result = temp_result\n",
    "    pbar.close()\n",
    "    \n",
    "    with open(pkl_filename, 'wb') as f:\n",
    "        pkl.dump(result, f)\n",
    "\n",
    "else:\n",
    "    assert os.path.exists(pkl_filename)\n",
    "    print(\"Loading data from pickle file {}\".format(pkl_filename))\n",
    "    with open(pkl_filename, 'rb') as f:\n",
    "        result = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_timestamp_to_plottable_datetime(data):\n",
    "    \"\"\"\n",
    "    Convert float UTC timestamp to equivalent type that is plottable by matplotlib\n",
    "\n",
    "    :param data: Pandas series of float timestamps\n",
    "    :type data: pandas.Series\n",
    "    :return: Array of Python datetimes\n",
    "    :rtype: numpy.array(datetime)\n",
    "    \"\"\"\n",
    "    return data.transform(datetime.datetime.utcfromtimestamp).astype('datetime64[ms]').dt.to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame(np.array(result), columns=['net', 'sta', 'loc', 'cha', 'date_utc', 'num_traces'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert obspy dates to sortable and matplotlib compatible dates\n",
    "# date_plottable = df_all['date'].transform(utc_time_to_plottable_datetime)\n",
    "date_plottable = df_all['date_utc'].transform(float)\n",
    "df_all['date_flt'] = date_plottable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out earlier historical deployments duplicating certain network codes\n",
    "if True:\n",
    "    # Some select stations require custom date filters to remove singular events outside \n",
    "    # the date range of the rest of the network.\n",
    "    DATE_FILTER = (\n",
    "        ('7D', pd.Timestamp(datetime.datetime(2010, 1, 1))), \n",
    "        ('7G', pd.Timestamp(datetime.datetime(2010, 1, 1)))\n",
    "    )\n",
    "    before = len(df_all)\n",
    "    for net, min_date in DATE_FILTER:\n",
    "        date_mask = (df_all['net'] == net) & (df_all['date_flt'] < min_date.timestamp())\n",
    "        df_all = df_all[~date_mask]\n",
    "    after = len(df_all)\n",
    "    print('Removed {} events due to timestamps'.format(before - after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "df_all = df_all.sort_values(['net', 'sta', 'cha', 'date_flt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all stations\n",
    "all_pairs = [(n, s) for (n, s), _ in df_all.groupby(['net', 'sta'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_station_uptime(df, netcode, statcode):\n",
    "    # Fixed properties\n",
    "    barprops = dict(aspect='auto', cmap='RdYlGn', interpolation='bilinear', alpha=0.8)\n",
    "    \n",
    "    sta_mask = (df['net'] == netcode) & (df['sta'] == statcode)\n",
    "    if not np.any(sta_mask):\n",
    "        return\n",
    "    df_sta = df.loc[sta_mask]\n",
    "    loc_cha = set()\n",
    "    for (l, s) in df_sta[['loc', 'cha']].values:\n",
    "        loc_cha.add((l, s))\n",
    "    loc_cha = sorted(list(loc_cha))\n",
    "    num_plots = len(loc_cha)\n",
    "    fig_height = 1*num_plots + 2\n",
    "    plt.figure(figsize=(16, fig_height))\n",
    "\n",
    "    # Make sure we have a common time axis for all subplots.\n",
    "    stn_min_date = df_sta['date_flt'].min()\n",
    "    stn_max_date = df_sta['date_flt'].max()\n",
    "    if netcode in TEMP_DEPLOYMENTS:\n",
    "        deployment_dates = TEMP_DEPLOYMENTS[netcode]\n",
    "        min_date = float(deployment_dates[0])\n",
    "        max_date = float(deployment_dates[1])\n",
    "    else:\n",
    "        min_date = stn_min_date\n",
    "        max_date = stn_max_date\n",
    "\n",
    "    for i, (loc, cha) in enumerate(loc_cha):\n",
    "        mask = (df_sta['loc'] == loc) & (df_sta['cha'] == cha)\n",
    "        if num_plots > 1:\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "        df_masked = df_sta.loc[mask]\n",
    "        data_avail = np.where(df_masked['num_traces'] > 0, 1.0, 0.0)\n",
    "        im = plt.imshow(data_avail.reshape((1, -1)), **barprops)\n",
    "        plt.ylabel(cha, fontsize=12)\n",
    "        if i < len(loc_cha) - 1:\n",
    "            plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlim(min_date, max_date)\n",
    "        extent = im.get_extent()\n",
    "        im.set_extent((df_masked['date_flt'].iloc[0], df_masked['date_flt'].iloc[-1], extent[2], extent[3]))\n",
    "\n",
    "    if num_plots > 1:\n",
    "        title_func = plt.suptitle\n",
    "    else:\n",
    "        title_func = plt.title\n",
    "    title_func('.'.join([netcode, statcode]) + ' [green=up, red=down]', fontsize=16)\n",
    "    xticks = [min_date]\n",
    "    if stn_min_date != min_date:\n",
    "        xticks.extend([stn_min_date])\n",
    "    xticks.extend(plt.xticks()[0])\n",
    "    if stn_max_date != max_date:\n",
    "        xticks.extend([stn_max_date])\n",
    "    xticks.extend([max_date])\n",
    "    xticks = sorted(xticks)\n",
    "    plt.xticks(xticks)\n",
    "    plt.xlim(min_date, max_date)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.gca().xaxis.set_major_formatter(\n",
    "        mtick.FuncFormatter(lambda pos, _: time.strftime(\"%Y-%m-%d\",time.localtime(pos))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SAVE_FIGS = True\n",
    "output_folder = 'survey'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "pbar = tqdm(total=len(all_pairs))\n",
    "for n, s in all_pairs:\n",
    "    full_code = '.'.join([n, s])\n",
    "    pbar.update()\n",
    "    pbar.set_description(full_code)\n",
    "    plot_station_uptime(df_all, n, s)\n",
    "    if SAVE_FIGS:\n",
    "        outfile = os.path.join(output_folder, full_code + \"_survey.png\")\n",
    "        plt.savefig(outfile, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
