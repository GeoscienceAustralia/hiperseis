{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import rf\n",
    "import rf.imaging\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import hilbert\n",
    "from scipy.stats import moment\n",
    "from scipy.interpolate import interp1d\n",
    "import obspy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in interactive widgets capability. See https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "# from beakerx import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = r\"..\\DATA\\OA_event_waveforms_for_rf_20170911T000036-20181128T230620_LQT_td_rev3_qual.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oa_all = rf.read_rf(src_file, format='h5', group='/waveforms/OA.BT23.0M')\n",
    "oa_all = rf.read_rf(src_file, format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# oa_copy = copy.deepcopy(oa_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oa_copy.moveout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,9))\n",
    "# time_offset = oa_all[0].stats.onset - oa_all[0].stats.starttime\n",
    "# plt.plot(oa_all[0].times() - time_offset, oa_all[0].data)\n",
    "# time_offset = oa_copy[0].stats.onset - oa_copy[0].stats.starttime\n",
    "# plt.plot(oa_copy[0].times() - time_offset, oa_copy[0].data, '--')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define useful internal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_to_dict(rf_data):\n",
    "    db = defaultdict(lambda: defaultdict(list))\n",
    "    for s in rf_data:\n",
    "        _, sta, _, cha = s.id.split('.')\n",
    "        db[sta][cha].append(s)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_station_rf_overlays(db_station, title=None):\n",
    "    num_channels = 0\n",
    "    for ch, traces in db_station.items():\n",
    "        if traces:\n",
    "            num_channels += 1\n",
    "\n",
    "    plt.figure(figsize=(16, 8*num_channels))\n",
    "    colors = [\"#8080a040\", \"#80a08040\", \"#a0808040\"]\n",
    "    min_x = 1e+20\n",
    "    max_x = -1e20\n",
    "\n",
    "    signal_means = []\n",
    "    for i, (ch, traces) in enumerate(db_station.items()):\n",
    "        if not traces:\n",
    "            continue\n",
    "        col = colors[i]\n",
    "        plt.subplot(num_channels, 1, i + 1)\n",
    "        sta = traces[0].stats.station\n",
    "        for j, tr in enumerate(traces):\n",
    "            lead_time = tr.stats.onset - tr.stats.starttime\n",
    "            times = tr.times()\n",
    "            plt.plot(times - lead_time, tr.data, '--', color=col, linewidth=2)\n",
    "            mask = (~np.isnan(tr.data) & ~np.isinf(tr.data))\n",
    "            if j == 0:\n",
    "                data_mean = np.zeros_like(tr.data)\n",
    "                data_mean[mask] = tr.data[mask]\n",
    "                counts = mask.astype(np.float)\n",
    "            else:\n",
    "                data_mean[mask] += tr.data[mask]\n",
    "                counts += mask.astype(np.float)\n",
    "            # end if\n",
    "        # end for\n",
    "        data_mean = data_mean/counts\n",
    "        data_mean[(counts == 0)] = np.nan\n",
    "        signal_means.append(data_mean)\n",
    "        plt.plot(tr.times() - lead_time, data_mean, color=\"#202020\", linewidth=2)\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude (normalized)')\n",
    "        plt.grid(linestyle=':', color=\"#80808020\")\n",
    "        title_text = '.'.join([sta, ch])\n",
    "        if title is not None:\n",
    "            title_text += ' ' + title\n",
    "        plt.title(title_text, fontsize=14)\n",
    "        x_lims = plt.xlim()\n",
    "        min_x = min(min_x, x_lims[0])\n",
    "        max_x = max(max_x, x_lims[1])\n",
    "    # end for\n",
    "    for i in range(num_channels):\n",
    "        subfig = plt.subplot(num_channels, 1, i + 1)\n",
    "        subfig.set_xlim((min_x, max_x))\n",
    "    # end for\n",
    "    \n",
    "    return signal_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_nth_root(arr, order):\n",
    "    if order == 1:\n",
    "        return arr\n",
    "    else:\n",
    "        return np.sign(arr)*np.power(np.abs(arr), 1.0/order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_nth_power(arr, order):\n",
    "    if order == 1:\n",
    "        return arr\n",
    "    else:\n",
    "        return np.sign(arr)*np.power(np.abs(arr), order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hk_stack(db_station, cha, h_range=np.linspace(10.0, 70.0, 301), k_range = np.linspace(1.3, 2.1, 201),\n",
    "                     V_p = 6.4, root_order=1, include_t3=True):\n",
    "\n",
    "    # Pre-compute grid quantities\n",
    "    k_grid, h_grid = np.meshgrid(k_range, h_range)\n",
    "    hk_stack = np.zeros_like(k_grid)\n",
    "    H_on_V_p = h_grid/V_p\n",
    "    k2 = k_grid*k_grid\n",
    "\n",
    "    stream_stack = []\n",
    "    cha_data = db_station[cha]\n",
    "    # Loop over streams, compute times, and stack interpolated values at those times\n",
    "    for s in cha_data:\n",
    "        incidence = s.stats.inclination\n",
    "        incidence_rad = incidence*np.pi/180.0\n",
    "        cos_i, sin_i = np.cos(incidence_rad), np.sin(incidence_rad)\n",
    "        sin2_i = sin_i*sin_i\n",
    "        term1 = H_on_V_p*k_grid*np.abs(cos_i)\n",
    "        term2 = H_on_V_p*np.sqrt(1 - k2*sin2_i)\n",
    "        # Time for Ps\n",
    "        t1 = term1 - term2\n",
    "        # Time for PpPs\n",
    "        t2 = term1 + term2\n",
    "        if include_t3:\n",
    "            # Time for PpSs + PsPs\n",
    "            t3 = 2*term1\n",
    "\n",
    "        # Subtract lead time so that primary P-wave arrival is at time zero.\n",
    "        lead_time = s.stats.onset - s.stats.starttime\n",
    "        times = s.times() - lead_time\n",
    "        # Create interpolator from stream signal for accurate time sampling.\n",
    "        interpolator = interp1d(times, s.data, kind='linear', copy=False, bounds_error=False, assume_sorted=True)\n",
    "\n",
    "        phase_sum = []\n",
    "        phase_sum.append(signed_nth_root(interpolator(t1), root_order))\n",
    "        phase_sum.append(signed_nth_root(interpolator(t2), root_order))\n",
    "        if include_t3:\n",
    "            # Negative sign on the third term is intentional, see Chen et al. (2010) and Zhu & Kanamori (2000).\n",
    "            # It needs to be negative because the PpSs + PsPs peak has negative phase,\n",
    "            # see http://eqseis.geosc.psu.edu/~cammon/HTML/RftnDocs/rftn01.html\n",
    "            # Apply nth root technique to reduce uncorrelated noise (Chen et al. (2010))\n",
    "            phase_sum.append(-signed_nth_root(interpolator(t3), root_order))\n",
    "\n",
    "        stream_stack.append(phase_sum)\n",
    "    # end for\n",
    "\n",
    "    # Perform the stacking (sum) across streams. hk_stack retains separate t1, t2, and t3 components here.\n",
    "    hk_stack = np.nanmean(np.array(stream_stack), axis=0)\n",
    "\n",
    "    # This inversion of the nth root is different to Sippl and Chen, but consistent with Muirhead\n",
    "    # who proposed the nth root technique. It improves the contrast of the resulting plot.\n",
    "    if root_order != 1:\n",
    "        hk_stack = signed_nth_power(hk_stack, root_order)\n",
    "\n",
    "    return k_grid, h_grid, hk_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_stack(hk_components, weighting=(0.5, 0.5, 0.0)):\n",
    "    assert hk_components.shape[0] == len(weighting), hk_components.shape\n",
    "    hk_phase_stacked = np.dot(np.moveaxis(hk_components, 0, -1), np.array(weighting))\n",
    "    return hk_phase_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hk_stack(k_grid, h_grid, hk_stack, title=None, save_file=None, show=True, num=None, clip_negative=True):\n",
    "    # Call computed_weighted_stack() first to combine weighted components before calling this function.\n",
    "    # Use a perceptually linear color map.\n",
    "    colmap = 'plasma'\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    if clip_negative:\n",
    "        hk_stack[hk_stack < 0] = 0\n",
    "    plt.contourf(k_grid, h_grid, hk_stack, levels=50, cmap=colmap)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.set_ylabel('Stack sum')\n",
    "    plt.contour(k_grid, h_grid, hk_stack, levels=10, colors='k', linewidths=1)\n",
    "    plt.xlabel(r'$\\kappa = \\frac{V_p}{V_s}$ (ratio)', fontsize=14)\n",
    "    plt.ylabel('H = Moho depth (km)', fontsize=14)\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    if num is not None:\n",
    "        xl = plt.xlim()\n",
    "        yl = plt.ylim()\n",
    "        txt_x = xl[0] + 0.85*(xl[1] - xl[0])\n",
    "        txt_y = yl[0] + 0.95*(yl[1] - yl[0])\n",
    "        plt.text(txt_x, txt_y, \"N = {}\".format(num), color=\"#ffffff\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    if save_file is not None:\n",
    "        tries = 10\n",
    "        while tries > 0:\n",
    "            try:\n",
    "                tries -= 1\n",
    "                plt.savefig(save_file, dpi=300)\n",
    "                break\n",
    "            except PermissionError:\n",
    "                time.sleep(1)\n",
    "                if tries == 0:\n",
    "                    print(\"WARNING: Failed to save file {} due to permissions!\".format(save_file))\n",
    "                    break\n",
    "            # end try\n",
    "        # end while\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_station_streams(db_station, freq_band=(None, None)):\n",
    "    \"\"\"Perform frequency filtering on streams. Returns a replica of db_station with streams containing filtered results.\n",
    "    \"\"\"\n",
    "    db_station_filt = defaultdict(list)\n",
    "    for i, (ch, streams) in enumerate(db_station.items()):\n",
    "        if ch == 'size' or i >= 3:\n",
    "            continue\n",
    "        for j, s in enumerate(streams):\n",
    "            stream_filt = s.copy()\n",
    "            if freq_band[0] is None and freq_band[1] is not None:\n",
    "                stream_filt.filter('lowpass', zerophase=True, corners=2, freq=freq_band[1])\n",
    "            elif freq_band[0] is not None and freq_band[1] is None:\n",
    "                stream_filt.filter('highpass', zerophase=True, corners=2, freq=freq_band[0])\n",
    "            elif freq_band[0] is not None and freq_band[1] is not None:\n",
    "                stream_filt.filter('bandpass', zerophase=True, corners=2, freqmin=freq_band[0],\n",
    "                                   freqmax=freq_band[1])\n",
    "            # end if\n",
    "            db_station_filt[ch].append(stream_filt)\n",
    "        # end for\n",
    "    # end for\n",
    "\n",
    "    return db_station_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def filter_station_to_mean_signal(db_station, min_correlation=1.0):\n",
    "    \"\"\"Filter out streams which are not 'close enough' to the mean signal,\n",
    "       based on simple correlation score.\n",
    "    \"\"\"\n",
    "    # Compute mean signals of channels in station\n",
    "    mean_rfs = []\n",
    "    for i, (ch, streams) in enumerate(db_station.items()):\n",
    "        if ch == 'size' or i >= 3:\n",
    "            continue\n",
    "        for j, s in enumerate(streams):\n",
    "            if j == 0:\n",
    "                data_mean = copy.deepcopy(s.data)\n",
    "            else:\n",
    "                data_mean += s.data\n",
    "            # end if\n",
    "        # end for\n",
    "        data_mean /= np.max(data_mean)\n",
    "        mean_rfs.append(data_mean)\n",
    "    # end for\n",
    "\n",
    "    # Filter out signals that do not meet minimum coherence with mean signal for each channel\n",
    "    db_station_filt = defaultdict(list)\n",
    "\n",
    "    corrs = []\n",
    "    for i, (ch, streams) in enumerate(db_station.items()):\n",
    "        for j, s in enumerate(streams):\n",
    "            corr = np.dot(s.data, mean_rfs[i])/np.dot(mean_rfs[i], mean_rfs[i])\n",
    "            if corr >= min_correlation:\n",
    "                db_station_filt[ch].append(s)\n",
    "            corrs.append(corr)\n",
    "        # end for\n",
    "    # end for\n",
    "                \n",
    "    return db_station_filt, corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rf_stack(rf_stream, time_window=(-10.0, 25.0), trace_height=0.2, stack_height=0.8, save_file=None):\n",
    "    _ = rf_stream.plot_rf(fillcolors=('#000000', '#a0a0a0'), trim=time_window, trace_height=trace_height,\n",
    "                          stack_height=stack_height, fname=save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_metadata(db_station):\n",
    "    for ch, traces in db_station.items():\n",
    "        for tr in traces:\n",
    "            rms_amp = np.sqrt(np.mean(np.square(tr.data)))\n",
    "            cplx_amp = np.abs(hilbert(tr.data))\n",
    "            mean_cplx_amp = np.mean(cplx_amp)\n",
    "            amp_20pc = np.percentile(cplx_amp, 20)\n",
    "            amp_80pc = np.percentile(cplx_amp, 80)\n",
    "            tr.stats.rms_amp = rms_amp\n",
    "            tr.stats.mean_cplx_amp = mean_cplx_amp\n",
    "            tr.stats.amp_20pc = amp_20pc\n",
    "            tr.stats.amp_80pc = amp_80pc\n",
    "        # end for\n",
    "    # end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert RFStream to dict database for convenient iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(oa_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = rf_to_dict(oa_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# sta_codes = list(db.keys())\n",
    "# sta_lat = [db[sta]['HHR'][0].stats.station_latitude for sta in sta_codes]\n",
    "# sta_lon = [db[sta]['HHR'][0].stats.station_longitude for sta in sta_codes]\n",
    "# df_OA_sta = pd.DataFrame.from_dict({'Station': sta_codes, 'Latitude': sta_lat, 'Longitude': sta_lon})\n",
    "# df_OA_sta.to_csv('OA_stations.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get coordinates of select stations for north-south lines\n",
    "# for sta in ['BV21', 'BV28', 'BX20', 'BX28', 'BY20', 'BY28', 'CB20', 'CB28', 'CD21', 'CD28']:\n",
    "#     lat = db[sta]['HHR'][0].stats.station_latitude\n",
    "#     lon = db[sta]['HHR'][0].stats.station_longitude\n",
    "#     print(\"{}: ({} {})\".format(sta, lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get coordinates of select stations for west-east lines\n",
    "# for sta in ['BU22', 'CI22', 'BS26', 'CE26', 'BS28', 'CF28']:\n",
    "#     lat = db[sta]['HHR'][0].stats.station_latitude\n",
    "#     lon = db[sta]['HHR'][0].stats.station_longitude\n",
    "#     print(\"{}: ({} {})\".format(sta, lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get coordinates of select stations for Andrew Clark\n",
    "# for i, sta_pair in enumerate([('BV21', 'CC28'), ('BX20', 'BX28'), ('BY20', 'BY28')]):\n",
    "#     if i == 0:\n",
    "#         # NE-->SW line\n",
    "#         lat0 = db[sta_pair[0]]['HHR'][0].stats.station_latitude + 0.1\n",
    "#         lon0 = db[sta_pair[0]]['HHR'][0].stats.station_longitude - 0.1\n",
    "#         lat1 = db[sta_pair[1]]['HHR'][0].stats.station_latitude - 0.1\n",
    "#         lon1 = db[sta_pair[1]]['HHR'][0].stats.station_longitude + 0.1\n",
    "#     else:\n",
    "#         # N-->S line\n",
    "#         lat0 = db[sta_pair[0]]['HHR'][0].stats.station_latitude + 0.1\n",
    "#         lon0 = db[sta_pair[0]]['HHR'][0].stats.station_longitude\n",
    "#         lat1 = db[sta_pair[1]]['HHR'][0].stats.station_latitude - 0.1\n",
    "#         lon1 = db[sta_pair[1]]['HHR'][0].stats.station_longitude\n",
    "#     print(\"{}-{}: --start-latlon {} {} --end-latlon {} {}\".format(sta_pair[0], sta_pair[1], lat0, lon0, lat1, lon1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select test station and channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_station = 'BT23'\n",
    "oa_test = db[test_station]\n",
    "# oa_test = db['BS27']\n",
    "# oa_test = db['BZ20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'HHQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oa_test[channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([np.any(np.isnan(tr.data)) for tr in oa_test[channel]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional statistics for discrimination between trace quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_new_metadata(oa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine available metadata in each trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(oa_test[channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(oa_test[channel][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_test[channel][0].stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display ranges of metadata and quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_series(traces, field):\n",
    "    x = [tr.stats.get(field) for tr in traces]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata and quality data on all traces for the target channel\n",
    "snr = get_metadata_series(oa_test[channel], 'snr')\n",
    "entropy = get_metadata_series(oa_test[channel], 'entropy')\n",
    "coherence = get_metadata_series(oa_test[channel], 'max_coherence')\n",
    "distance = get_metadata_series(oa_test[channel], 'distance')\n",
    "inclination = get_metadata_series(oa_test[channel], 'inclination')\n",
    "magnitude = get_metadata_series(oa_test[channel], 'event_magnitude')\n",
    "depth = get_metadata_series(oa_test[channel], 'event_depth')\n",
    "amax = get_metadata_series(oa_test[channel], 'amax')\n",
    "amp_20pc = get_metadata_series(oa_test[channel], 'amp_20pc')\n",
    "amp_80pc = get_metadata_series(oa_test[channel], 'amp_80pc')\n",
    "mean_cplx_amp = get_metadata_series(oa_test[channel], 'mean_cplx_amp')\n",
    "rf_group = get_metadata_series(oa_test[channel], 'rf_group')\n",
    "rms_amp = get_metadata_series(oa_test[channel], 'rms_amp')\n",
    "# Replace no-group group IDs with '-1'\n",
    "rf_group = [g if g is not None else -1 for g in rf_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_array = [(snr, \"SNR\"), (entropy, \"Entropy\"), (coherence, \"Coherence\"), (distance, \"Distance\"),\n",
    "              (inclination, \"Inclination\"), (magnitude, \"Magnitude\"), (amax, \"Max amplitude\"), (amp_20pc, \"Amplitude 20th perc.\"),\n",
    "              (amp_80pc, \"Amplitude 80th perc.\"), (mean_cplx_amp, \"Mean amplitude\"), (rms_amp, \"RMS amplitude\"), (rf_group, \"Group ID\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(4,3,1)\n",
    "for i, (data, name) in enumerate(dist_array):\n",
    "    ax = plt.subplot(4, 3, i + 1)\n",
    "#     plt.hist(data, bins=20)\n",
    "    sns.distplot(data, bins=20, ax=ax)\n",
    "    plt.title(name + \" distribution\", y=0.88, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine co-plots to look for discriminating variables\n",
    "df = pd.DataFrame.from_dict({\"SNR\": snr, \"Entropy\": entropy, \"Coherence\": coherence, \"Max_amp\": amax,\n",
    "                             \"Amp_20pc\": amp_20pc, \"Amp_80pc\": amp_80pc, \"RMS_amp\": rms_amp, \"Mean_amp\": mean_cplx_amp,\n",
    "                             \"Magnitude\": \">=6\", \"Distance\": \">=60\", \"Depth\": \">=80km\",\n",
    "                             \"Inclination\": \">=20\", \"Group_id\": rf_group,\n",
    "                             \"Quality\": \"unknown\"})\n",
    "df.loc[(np.array(magnitude) < 6.0), \"Magnitude\"] = \"<6\"\n",
    "df.loc[(np.array(distance) < 60.0), \"Distance\"] = \"<60\"\n",
    "df.loc[(np.array(inclination) < 20.0), \"Inclination\"] = \"<20\"\n",
    "df.loc[(np.array(depth) < 80.0), \"Depth\"] = \"<80km\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_file = test_station + \"_quality.csv\"\n",
    "if os.path.isfile(qual_file):\n",
    "    loaded_quality = pd.read_csv(qual_file)\n",
    "    df['Quality'] = loaded_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use interactive widget to manually label the quality of the traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quality guide:\")\n",
    "print(\"'a' = low signal before onset, higher signal after onset with some multiples visible\")\n",
    "print(\"'b' = signal similar before and after onset, cannot make out multiples with much confidence\")\n",
    "print(\"Create labels by entering 10 character string of 'a's and 'b's according to quality, ordered from bottom to top trace.\")\n",
    "# Create labels for quality. Note that rf plots are numbered from the bottom up, whereas the Pandas table is displayed ordered from the top down.\n",
    "quality_updated = False\n",
    "for i in range(0, len(df), 10):\n",
    "    existing_qual = df['Quality'].iloc[i:i+10].values\n",
    "    if not 'unknown' in existing_qual:\n",
    "        continue\n",
    "    rf_slice = rf.RFStream(oa_test[channel][i:i+10])\n",
    "    plot_rf_stack(rf_slice, trace_height=0.4)\n",
    "    plt.show()\n",
    "    get_labels = ''\n",
    "    quit = False\n",
    "    while len(get_labels) != len(rf_slice):\n",
    "        get_labels = input(\"Enter labels: \")\n",
    "        if get_labels.lower() == 'quit':\n",
    "            quit = True\n",
    "            break\n",
    "        if len(get_labels) != len(rf_slice):\n",
    "            print(\"Wrong number of labels, try again!\")\n",
    "    if quit:\n",
    "        break\n",
    "    for j, qual in enumerate(get_labels):\n",
    "        df['Quality'].iloc[i+j] = qual\n",
    "    quality_updated = True\n",
    "    display(df.iloc[i:i+10])\n",
    "\n",
    "if quality_updated:\n",
    "    df['Quality'].to_csv(qual_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign quality category to trace metadata\n",
    "for i, tr in enumerate(oa_test[channel]):\n",
    "    tr.stats.quality = df['Quality'].iloc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot labelled data to find metrics to discriminate trace quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual\n",
    "def metrics_pairplot(hue_by=['Quality', 'Magnitude', 'Distance', 'Depth', 'Inclination', 'Group_id']):\n",
    "    hue_order = None\n",
    "    if hue_by == 'Quality':\n",
    "        hue_order = ['unknown', 'b', 'a'] if 'unknown' in df['Quality'] else ['b', 'a']\n",
    "    sns.pairplot(df, hue=hue_by, hue_order=hue_order, vars=[\"SNR\", \"Entropy\", \"Coherence\", \"Max_amp\", \"Amp_20pc\", \"Amp_80pc\", \"RMS_amp\", \"Mean_amp\"])\n",
    "    plt.suptitle(\"Pairwise quality metrics scatter plot\", y=1.01, fontsize=20)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at how effective selected metadata metrics are at filtering to the Quality A set of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total = len(oa_test[channel])\n",
    "\n",
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.quality == 'a']\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream_A = rf.RFStream(rf_data)\n",
    "print(\"Quality A: {} events\".format(len(rf_stream_A)))\n",
    "quality_A_ids = [tr.stats.event_id for tr in rf_stream_A]\n",
    "not_quality_A_ids = [tr.stats.event_id for tr in oa_test[channel] if tr.stats.event_id not in quality_A_ids]\n",
    "\n",
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.snr >= 1.5 and tr.stats.entropy >= 3.0 and tr.stats.max_coherence >= 0.15]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream_stats_filtered = rf.RFStream(rf_data)\n",
    "num_filtered = len(rf_stream_stats_filtered)\n",
    "print(\"Stats filtered: {} events\".format(num_filtered))\n",
    "stats_filtered_ids = [tr.stats.event_id for tr in rf_stream_stats_filtered]\n",
    "true_positives = [id for id in stats_filtered_ids if id in quality_A_ids]\n",
    "false_negatives = [id for id in quality_A_ids if id not in stats_filtered_ids]\n",
    "num_true_positive = len(true_positives)\n",
    "num_false_negative = len(false_negatives)\n",
    "num_predicted_positive = len(stats_filtered_ids)\n",
    "num_predicted_negative = num_total - num_predicted_positive\n",
    "\n",
    "# Determine how many of the events in stats_filtered_ids are Quality A events\n",
    "print(\"{}/{} correct filtered events (snr, entropy, coherence) (Positive predictive value = {:.2f}%, False omission rate = {:.2f}%)\"\n",
    "      .format(num_qual_A, num_filtered, 100.0*num_true_positive/num_predicted_positive, 100*num_false_negative/num_predicted_negative))\n",
    "\n",
    "# Repeat using amplitude metrics\n",
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.amax <= 0.3 and tr.stats.amp_20pc <= 0.03 and tr.stats.amp_80pc <= 0.1]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream_stats2_filtered = rf.RFStream(rf_data)\n",
    "num2_filtered = len(rf_stream_stats2_filtered)\n",
    "print(\"Stats2 filtered: {} events\".format(num2_filtered))\n",
    "stats2_filtered_ids = [tr.stats.event_id for tr in rf_stream_stats2_filtered]\n",
    "true_positives = [id for id in stats2_filtered_ids if id in quality_A_ids]\n",
    "false_negatives = [id for id in quality_A_ids if id not in stats2_filtered_ids]\n",
    "num_true_positive = len(true_positives)\n",
    "num_false_negative = len(false_negatives)\n",
    "num_predicted_positive = len(stats2_filtered_ids)\n",
    "num_predicted_negative = num_total - num_predicted_positive\n",
    "\n",
    "print(\"{}/{} filtered events (Max. amp, 20%, 80%) are quality A events (Positive predictive value = {:.2f}%, False omission rate = {:.2f}%)\"\n",
    "      .format(num2_qual_A, num2_filtered, 100.0*num_true_positive/num_predicted_positive, 100*num_false_negative/num_predicted_negative))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot RFs for traces filtered by various quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rf_stack(rf_stream_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.quality == 'b']\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "plot_rf_stack(rf_stream[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(rf)\n",
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.snr >= 3.0]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "# plot_rf_stack(rf_stream, save_file=\"sample_rf_aligners.png\")\n",
    "plot_rf_stack(rf_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.snr <= 0.8]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "plot_rf_stack(rf_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.entropy <= 3.0]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "plot_rf_stack(rf_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.entropy >= 4.2]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "plot_rf_stack(rf_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.max_coherence >= 0.3]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "plot_rf_stack(rf_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.max_coherence <= 0.02]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "plot_rf_stack(rf_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.event_magnitude >= 5.5]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "plot_rf_stack(rf_stream[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = [tr for tr in oa_test[channel] if tr.stats.event_magnitude < 5.5]\n",
    "rf_data = sorted(rf_data, key=lambda v: v.stats.back_azimuth)\n",
    "rf_stream = rf.RFStream(rf_data)\n",
    "plot_rf_stack(rf_stream[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot overlay of all traces in test channel (no filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_quality = {channel: [tr for tr in rf_stream_A]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traces = len(oa_quality[channel])\n",
    "trace_mean = plot_station_rf_overlays(oa_quality, '(all {} traces)'.format(num_traces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split traces into groups and plot each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dict = {}\n",
    "for tr in oa_quality[channel]:\n",
    "    grp = tr.stats.get('rf_group')\n",
    "    if grp is not None:\n",
    "        if grp in group_dict:\n",
    "            group_dict[grp][channel].append(tr)\n",
    "        else:\n",
    "            group_dict[grp] = {}\n",
    "            group_dict[grp][channel] = [tr]\n",
    "\n",
    "groups = group_dict.keys()\n",
    "print(\"Found {} groups: {}\".format(len(groups), groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp_id, group in group_dict.items():\n",
    "    num_traces = len(group[channel])\n",
    "    title = '(group {}, {} traces)'.format(grp_id, num_traces)\n",
    "    group_mean = plot_station_rf_overlays(group, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot only traces with similarity to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_quality_filt, corrs = filter_station_to_mean_signal(oa_quality, min_correlation=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(corrs, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traces = len(oa_quality_filt[channel])\n",
    "test_filt_mean = plot_station_rf_overlays(oa_quality_filt, '({} traces similar to mean)'.format(num_traces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate the effectiveness of phase-weighting the traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismic.receiver_fn.rf_util import phase_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = phase_weights(oa_quality_filt[channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = oa_quality_filt[channel][0]\n",
    "time_offset = s0.stats.onset - s0.stats.starttime\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(s0.times() - time_offset, pw)\n",
    "plt.title('Phase weightings')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of phase weighting to suppress areas where phases tend to be random.\n",
    "pw_exponent = 2\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(s0.times() - time_offset, s0.data, linewidth=2)\n",
    "plt.plot(s0.times() - time_offset, s0.data*pw**pw_exponent, '--', linewidth=2)\n",
    "plt.legend(['Original', 'Phase weighted'])\n",
    "plt.title('Phase weighting applied to a single trace')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply phase weighting to data for H-k stacking\n",
    "# NOTE: This will overwrite the original filtered data\n",
    "for tr in oa_quality_filt[channel]:\n",
    "    tr.data = tr.data*pw**pw_exponent\n",
    "\n",
    "num_traces = len(oa_quality_filt[channel])\n",
    "test_filt_mean = plot_station_rf_overlays(oa_quality_filt, '({} traces similar to mean, phase weighted)'.format(num_traces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot HK stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_src_data = oa_quality_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stack\n",
    "weighting = (0.35, 0.35, 0.3)\n",
    "\n",
    "for cha in [channel]:\n",
    "    k_grid, h_grid, hk_stack = compute_hk_stack(hk_src_data, cha, root_order=2)\n",
    "\n",
    "    hk_stack_sum = compute_weighted_stack(hk_stack, weighting)\n",
    "    \n",
    "    sta = hk_src_data[cha][0].stats.station\n",
    "\n",
    "    num = len(hk_src_data[cha])\n",
    "    save_file = None\n",
    "    plot_hk_stack(k_grid, h_grid, hk_stack[0], title=sta + '.{} Ps'.format(cha), num=num)\n",
    "    plot_hk_stack(k_grid, h_grid, hk_stack[1], title=sta + '.{} PpPs'.format(cha), num=num)\n",
    "    plot_hk_stack(k_grid, h_grid, hk_stack[2], title=sta + '.{} PpSs + PsPs'.format(cha), num=num)\n",
    "    plot_hk_stack(k_grid, h_grid, hk_stack_sum, title=sta + '.{}'.format(cha) + ' (no filtering)', num=num, save_file=save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over all OA stations and plot HK-stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cha = channel\n",
    "# pbar = tqdm(total=len(db))\n",
    "# show = False\n",
    "# weighting = (0.5, 0.4, 0.1)\n",
    "# for sta, db_sta in db.items():\n",
    "#     pbar.set_description(sta)\n",
    "#     pbar.update()\n",
    "#     k_grid, h_grid, hk_stack = compute_hk_stack(db_sta, cha, root_order=2)\n",
    "#     hk_stack_sum = compute_weighted_stack(hk_stack, weighting)\n",
    "#     sta = db_sta[cha][0].stats.station\n",
    "#     save_file = sta + \"_{}_hk_stack.png\".format(cha)\n",
    "#     num = len(db_sta[cha])\n",
    "#     plot_hk_stack(k_grid, h_grid, hk_stack_sum, title=sta + '.{}'.format(cha), save_file=save_file, show=show, num=num)\n",
    "# pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
