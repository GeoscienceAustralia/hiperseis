#!/usr/bin/env python
"""Helper class to iterate over 3-channel event traces in h5 file generated by rf library,
   without loading all the traces into memory. This is a scalable solution for very large files.
"""

import logging

import numpy as np
import h5py
from obspyh5 import dataset2trace
from rf import RFStream

logging.basicConfig()


class IterRfH5FileEvents(object):
    """Helper class to iterate over events in h5 file generated by extract_event_traces.py and pass
       them to RF generator. This class avoids having to load the whole file up front via obspy which
       is slow and not scalable.
    """

    def __init__(self, h5_filename):
        self.h5_filename = h5_filename
        self.num_components = 3

    def __iter__(self):
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        logger.info("Scanning jobs metadata from file {}".format(self.h5_filename))
        with h5py.File(self.h5_filename, 'r') as f:
            wf_data = f['waveforms']
            num_stations = len(wf_data)
            count = 0
            event_count = 0
            for station_id in wf_data:
                count += 1
                logger.info("Station {} {}/{}".format(station_id, count, num_stations))
                station_data = wf_data[station_id]
                for event_time in station_data:
                    event_traces = station_data[event_time]
                    if len(event_traces) != self.num_components:
                        logging.warning("Incorrect number of traces ({}) for stn {} event {}, skipping"
                                        .format(len(event_traces), station_id, event_time))
                        continue
                    traces = []
                    skip_trace = False
                    for trace_id in event_traces:
                        trace = dataset2trace(event_traces[trace_id])
                        if np.any(np.isnan(trace.data)):
                            skip_trace = True
                            logging.error("Invalid trace data in {}, skipping".format(trace_id))
                            break
                        traces.append(trace)
                    if skip_trace:
                        continue
                    event_count += 1
                    stream = RFStream(traces=traces).sort()
                    yield stream
                # end for
            # end for
        # end with
        logger.info("Yielded {} event traces to process".format(event_count))
    # end func
