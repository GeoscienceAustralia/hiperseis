#!/usr/bin/env python
# coding=utf-8
"""
Analyze a data set of seismic arrival events on a per-station basis and try to
detect and estimate any station orientation error.

The event waveform dataset provided as input to this script (or to NetworkEventDataset
if calling directly into function `analyze_station_orientations`) is generated by the
script `seismic/extract_event_traces.py`.

In future, consider moving this script to the `inventory` module and applying
corrections to the station inventory xml (to the azimuth tag).

Reference:

- Wilde-Piórko, M., Grycuk, M., Polkowski, M. et al.
  On the rotation of teleseismic seismograms based on the receiver function technique.
  J Seismol 21, 857-868 (2017). https://doi.org/10.1007/s10950-017-9640-x
"""

import json
import click
import matplotlib.pyplot as plt
import os, copy
import numpy as np
from collections import defaultdict
from seismic.stream_quality_filter import curate_seismograms
from rf import RFStream
from seismic.receiver_fn.generate_rf_helper import transform_stream_to_rf

from seismic.network_event_dataset import NetworkEventDataset
from scipy import optimize, interpolate
import seismic.receiver_fn.rf_util as rf_util
from seismic.stream_io import get_obspyh5_index
import tqdm.auto as tqdm
import logging
from mpi4py import MPI

logging.basicConfig()

# pylint: disable=invalid-name

# Take care not to use any curation options that would vary if there were a station orientation error.
DEFAULT_CURATION_OPTS = {
    "min_snr": 2.0,
    "max_raw_amplitude": 20000.0,
    "rms_amplitude_bounds": {"R/Z": 1.0, "T/Z": 1.0}
}

DEFAULT_CONFIG_FILTERING = {
    "resample_rate": 10.0,
    "taper_limit": 0.05,
    "filter_band": [0.02, 1.0],
}

DEFAULT_CONFIG_PROCESSING = {
    "rotation_type": "ZRT",
    "deconv_domain": "time",
    "normalize": True,
    "trim_start_time": -10,
    "trim_end_time": 30,
    "spiking": 1.0
}

def analyze_station_orientations(ned, curation_opts=DEFAULT_CURATION_OPTS,
                                 config_filtering=DEFAULT_CONFIG_FILTERING,
                                 config_processing=DEFAULT_CONFIG_PROCESSING,
                                 save_plots_path=None, ax=None):
    """
    Main processing function for analyzing station orientation using 3-channel
    event waveforms. Uses method of Wilde-Piorko https://doi.org/10.1007/s10950-017-9640-x

    One should not worry about estimates that come back with error of less than
    about 20 degrees from zero, since this analysis provides only an estimate.

    :param ned: NetworkEventDataset containing waveforms to analyze. Note: the data in
        this dataset will be modified by this function.
    :type ned: seismic.network_event_dataset.NetworkEventDataset
    :param curation_opts: Seismogram curation options.
        Safe default to use is `DEFAULT_CURATION_OPTS`.
    :type curation_opts: dict
    :param config_filtering: Seismogram filtering options for RF computation.
        Safe default to use is `DEFAULT_CONFIG_FILTERING`.
    :type config_filtering: dict
    :param config_processing: Seismogram RF processing options.
        Safe default to use is `DEFAULT_CONFIG_PROCESSING`.
    :param parallel: Activates parallel processing through joblib
    type: boolean
    :type config_processing: dict
    :param save_plots_path: Optional folder in which to save plot per station of mean
        arrival RF amplitude as function of correction angle
    :type save_plots_path: str or pathlib.Path
    :return: Dict of estimated orientation error with net.sta code as the key.
    :rtype: dict
    """
    assert isinstance(ned, NetworkEventDataset), 'Pass NetworkEventDataset as input'

    if len(ned) == 0:
        return {}

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    # Determine limiting date range per station
    results = defaultdict(dict)
    for sta, db_evid in ned.by_station():
        start_time = end_time = None
        full_code = '.'.join([ned.network, sta])
        for _evid, stream in db_evid.items():
            if start_time is None:
                start_time = stream[0].stats.starttime
            # end if
            if end_time is None:
                end_time = stream[0].stats.endtime
            # end if
            for tr in stream:
                start_time = min(start_time, tr.stats.starttime)
                end_time = max(end_time, tr.stats.endtime)
            # end for
        # end for
        results[full_code]['date_range'] = [str(start_time), str(end_time)]
    # end for

    evids_orig = set([evid for _, evid, _ in ned])

    # Trim streams to time window
    logger.info('Trimming dataset')
    ned.apply(lambda stream: stream.trim(stream[0].stats.onset - 10, stream[0].stats.onset + 30))

    # Downsample.
    logger.info('Downsampling dataset')
    fs = 20.0
    ned.apply(lambda stream: stream.filter('lowpass', freq=fs/2.0, corners=2, zerophase=True).interpolate(fs, \
              method='lanczos', a=10) if len(stream) and stream[0].stats.sampling_rate > fs else stream)

    logger.info('Curating dataset')
    curate_seismograms(ned, curation_opts, logger, rotate_to_zrt=False)
    evids_to_keep = set([evid for _, evid, _ in ned])
    evids_discarded = evids_orig - evids_to_keep
    logger.info('Discarded {}/{} events'.format(len(evids_discarded), len(evids_orig)))

    logger.info('Analysing arrivals')
    angles = np.linspace(-180, 180, num=20, endpoint=False)

    stations = []
    sta_ampls = []
    for sta, db_evid in ned.by_station():
        stations.append((sta, len(db_evid)))
        sta_ampl = _run_single_station(db_evid, angles, config_filtering, config_processing)
        sta_ampls.append(sta_ampl)
    # end for

    sta_ori_metrics = [(sta, N, ampls) for (sta, N), ampls in zip(stations, sta_ampls)]

    x = np.hstack((angles, angles[0] + 360))
    angles_fine = np.linspace(-180, 180, num=361)
    for sta, N, ampls in sta_ori_metrics:
        y = np.array(ampls + [ampls[0]])
        mask = np.isfinite(y)
        n_valid = np.sum(mask)
        if n_valid < 5:
            logger.info('{}: Insufficient data ({} valid points)'.format(sta, n_valid))
            continue
        # end if
        x_valid = x[mask]
        y_valid = y[mask]
        #  Fit cubic spline through all points, used for visualization
        if np.isfinite(y[0]):
            bc_type = 'periodic'
        else:
            bc_type = 'not-a-knot'
        # end if
        interp_cubsp = interpolate.CubicSpline(x_valid, y_valid, bc_type=bc_type)
        yint_cubsp = interp_cubsp(angles_fine)
        # Fit cosine curve to all data points, used to estimate correction angle
        fitted, cov = optimize.curve_fit(lambda _x, _amp, _ph: _amp*np.cos(np.deg2rad(_x + _ph)),
                                         x_valid, y_valid, p0=[0.2, 0.0], bounds=([-1, -180], [1, 180]))
        A_fit, ph_fit = fitted
        y_fitted = A_fit*np.cos(np.deg2rad(angles_fine + ph_fit))
        ph_uncertainty = np.sqrt(cov[1, 1])
        # Estimate correction angle
        angle_max = angles_fine[np.argmax(y_fitted)]
        code = '.'.join([ned.network, sta])
        results[code]['azimuth_correction'] = angle_max
        logger.info('{}: {:2.3f}°, stddev {:2.3f}° (N = {:3d})'.format(
            sta, angle_max, ph_uncertainty, N))


        if save_plots_path is not None:
            _f = plt.figure(figsize=(16, 9))
            plt.plot(x_valid, y_valid, 'x', label='Computed P arrival strength')
            plt.plot(angles_fine, yint_cubsp, '--', alpha=0.7, label='Cubic spline fit')
            plt.plot(angles_fine, y_fitted, '--', alpha=0.7, label='Cosine fit')
            plt.text(angle_max, np.max(y_fitted), 'Max at: {} deg'.format(angle_max))
            plt.grid(linestyle=':', color="#80808080")
            plt.xlabel('Orientation correction (deg)', fontsize=12)
            plt.ylabel('P phase ampl. mean (0-1 sec)', fontsize=12)
            plt.title('{}.{}'.format(ned.network, sta), fontsize=14)
            plt.text(0.9, 0.9, 'N = {}'.format(N), ha='right', va='top',
                     transform=plt.gca().transAxes)
            plt.legend(framealpha=0.5, loc='lower left')
            outfile = '_'.join([code, config_processing["deconv_domain"], 'ori.png'])
            outfile = os.path.join(str(save_plots_path), outfile)
            plt.savefig(outfile, dpi=300)
            plt.close()
        elif(ax is not None):
            ax.plot(x_valid, y_valid, 'x', label='Computed P arrival strength')
            ax.plot(angles_fine, yint_cubsp, '--', alpha=0.7, label='Cubic spline fit')
            ax.plot(angles_fine, y_fitted, '--', alpha=0.7, label='Cosine fit')
            ax.text(angle_max, np.max(y_fitted), 'Max at: {} deg'.format(angle_max))
            ax.set_xlabel('Orientation correction (deg)', fontsize=12)
            ax.set_ylabel('P phase ampl. mean (0-1 sec)', fontsize=12)
            ax.text(0.9, 0.9, 'N = {}'.format(N), ha='right', va='top',
                    transform=ax.transAxes)
            ax.legend(framealpha=0.5, loc='lower left')
        # end if
    # end for

    return results
# end func

def _run_single_station(db_evid, angles, config_filtering, config_processing):
    """
    Internal processing function for running sequence of candidate angles
    over a single station.

    :param db_evid: Dictionary of event streams (3-channel ZNE) keyed by event ID. \
        Best obtained using class NetworkEventDataset
    :type db_evid: sortedcontainers.SortedDict or similar dict-like
    :param angles: Sequence of candidate correction angles to try (degrees)
    :type angles: Iterable(float)
    :param config_filtering: Waveform filtering options for RF processing
    :type config_filtering: dict
    :param config_processing: RF processing options
    :type config_processing: dict
    :return: Amplitude metric as a function of angle. Same length as angles array.
    :rtype: list(float)
    """
    ampls = []
    for correction in angles:
        rf_stream_all = RFStream()
        for evid, stream in db_evid.items():
            stream_rot = copy.deepcopy(stream)
            for tr in stream_rot:
                tr.stats.back_azimuth += correction
                while tr.stats.back_azimuth < 0:
                    tr.stats.back_azimuth += 360
                while tr.stats.back_azimuth >= 360:
                    tr.stats.back_azimuth -= 360
            # end for

            rf_3ch = transform_stream_to_rf(evid, RFStream(stream_rot),
                                            config_filtering, config_processing)
            if rf_3ch is None:
                continue

            rf_stream_all += rf_3ch
        # end for
        if len(rf_stream_all) > 0:
            rf_stream_R = rf_stream_all.select(component='R')
            rf_stream_R.trim2(-5, 5, reftime='onset')
            rf_stream_R.detrend('linear')
            rf_stream_R.taper(0.1)

            R_stack = rf_stream_R.stack().trim2(-1, 1, reftime='onset')[0].data
            ampl_mean = np.mean(R_stack)
        else:
            ampl_mean = np.nan
        # endif
        ampls.append(ampl_mean)
    # end for
    return ampls
# end func
